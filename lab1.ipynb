{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> LSA part: </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 1: Importing packages and corpus </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import rand\n",
    "from scipy.stats import uniform, pearsonr\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = brown.words()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 2: Extracting most common words and update list </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(raw_text):\n",
    "    word_check = re.compile('\\w+')\n",
    "    filter_non_word = re.compile('.*(\\W|\\d).*')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    processed_text = []\n",
    "\n",
    "    for w in raw_text:\n",
    "        w = w.lower()\n",
    "        if (word_check.match(w)) and (w not in stop_words) and (not filter_non_word.match(w)):\n",
    "            processed_text.append(w)\n",
    "            \n",
    "    return processed_text\n",
    "\n",
    "processed_t = preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency_dist(processed_text):\n",
    "    return nltk.FreqDist(w for w in processed_text)\n",
    "\n",
    "fdist = get_frequency_dist(processed_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Most common and least common words: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = fdist.most_common(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "\n",
    "for w in W:\n",
    "    vocab.append(w[0])\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent five words in the most frequent 5000 words in the Brown Corpus:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('one', 3292),\n",
       " ('would', 2714),\n",
       " ('said', 1961),\n",
       " ('new', 1635),\n",
       " ('could', 1601)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The most frequent five words in the most frequent 5000 words in the Brown Corpus:\")\n",
    "\n",
    "W[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least frequent five words in the most frequent 5000 words in the Brown Corpus:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('advances', 18),\n",
       " ('applicable', 18),\n",
       " ('humble', 18),\n",
       " ('defended', 18),\n",
       " ('spectacle', 18)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The least frequent five words in the most frequent 5000 words in the Brown Corpus:\")\n",
    "\n",
    "W[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Update existing list: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = ['grin', 'shore', 'gem', 'hill', 'pillow', 'cock', 'lad', 'cord', 'rooster', 'food', 'forest', 'autograph', 'monk', 'noon', 'bird', 'graveyard', 'woodland', 'car', 'crane', 'fruit', 'sage', 'cemetery', 'wizard', 'voyage', 'coast', 'implement', 'brother', 'automobile', 'journey', 'tool', 'stove', 'jewel', 'asylum', 'madhouse', 'string', 'rock', 'tumbler', 'oracle', 'cushion', 'smile', 'magician', 'boy', 'slave', 'mound', 'glass', 'serf', 'midday', 'furnace', 'signature']\n",
    "\n",
    "def add_words(old_w_list, new_w_list):\n",
    "    count = 0\n",
    "    min_count = fdist[old_w_list[-1][0]]\n",
    "    for w in set(new_w_list):\n",
    "        if w not in vocab and fdist[w] > 0:\n",
    "            old_w_list.append((tuple((w, fdist[w]))))\n",
    "\n",
    "add_words(W, new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5029\n"
     ]
    }
   ],
   "source": [
    "len_W = len(W)\n",
    "print(len_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5029\n"
     ]
    }
   ],
   "source": [
    "for w in W:\n",
    "    if w[0] not in vocab:\n",
    "        vocab.append(w[0])\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 3: Construct a word-context vector model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5029x5029 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = rand(len(vocab), len(vocab), density=0, format=\"csr\", random_state=0)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1 = matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509266"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq_dist = FreqDist()\n",
    "bigram_text = nltk.bigrams(processed_t)\n",
    "bigram_list = list(bigram_text)\n",
    "\n",
    "len(bigram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in bigram_list:\n",
    "    if b[0] in vocab and b[1] in vocab:\n",
    "        M1[vocab.index(b[1])][vocab.index(b[0])] += 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 4: Build PPMI Matrix: </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389739\n"
     ]
    }
   ],
   "source": [
    "def sum_unigram_freq(uni_W):\n",
    "    f_sum = 0\n",
    "    \n",
    "    for i in range(len(uni_W)):\n",
    "        f_sum += uni_W[i][1]\n",
    "        \n",
    "    return f_sum\n",
    "\n",
    "sum_uni_freq = sum_unigram_freq(W)\n",
    "print(sum_uni_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304578.0\n"
     ]
    }
   ],
   "source": [
    "def sum_bigram_freq(bigram_W):\n",
    "    f_sum = 0\n",
    "    \n",
    "    for i in range(len(bigram_W)):\n",
    "        for j in range(len(bigram_W[i])):\n",
    "            f_sum += bigram_W[i][j]\n",
    "            \n",
    "    return f_sum\n",
    "\n",
    "sum_bi_freq = sum_bigram_freq(M1)\n",
    "print(sum_bi_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(i_w1, i_w2, freq_joint, f_uni_sum, f_bi_sum, uni_W):\n",
    "    p_w1 = uni_W[i_w1][1] / float(f_uni_sum)\n",
    "    p_w2 = uni_W[i_w2][1] / float(f_uni_sum)\n",
    "    p_w1_w2 = freq_joint / float(f_bi_sum)\n",
    "    \n",
    "    try:\n",
    "        if p_w1*p_w2 == 0 or p_w1_w2/float(p_w1*p_w2) == 0.0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return max(math.log(p_w1_w2/float(p_w1*p_w2), 2), 0.0)\n",
    "    except:\n",
    "        0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1_plus = matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(M1_plus)):\n",
    "    for j in range(len(M1_plus)):\n",
    "        M1_plus[i][j] = ppmi(i, j, M1[i][j], sum_uni_freq, sum_bi_freq, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 5: Perform PCA: </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_PCA(ppmi_matrix, dim):\n",
    "    fit_transformer = StandardScaler().fit_transform(ppmi_matrix)\n",
    "    pca = PCA(n_components=dim)\n",
    "    return pca.fit_transform(ppmi_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2_10 = cal_PCA(M1_plus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(M2_10[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2_100 = cal_PCA(M1_plus, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(M2_100[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2_300 = cal_PCA(M1_plus, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(M2_300[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 6: Find all pairs of words in Table 1 and record the human-judged similarities: </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_dict = {('cord', 'smile'): 0.02, ('rooster', 'voyage'): 0.04, ('noon', 'string'): 0.04, \n",
    "    ('fruit', 'furnace'): 0.05, ('autograph', 'shore'): 0.06, ('automobile', 'wizard'): 0.11,\n",
    "    ('mound', 'stove'): 0.14, ('grin', 'implement'): 0.18, ('asylum', 'fruit'): 0.19,\n",
    "    ('asylum', 'monk'): 0.39, ('graveyard', 'madhouse'): 0.42, ('glass', 'magician'): 0.44,\n",
    "    ('boy', 'rooster'): 0.44, ('cushion', 'jewel'): 0.45, ('monk', 'slave'): 0.57,\n",
    "    ('asylum', 'cemetery'): 0.79, ('coast', 'forest'): 0.85, ('grin', 'lad'): 0.88,\n",
    "    ('shore', 'woodland'): 0.90, ('monk', 'oracle'): 0.91, ('boy', 'sage'): 0.96,\n",
    "    ('automobile', 'cushion'): 0.97, ('mound', 'shore'): 0.97, ('lad', 'wizard'): 0.99,\n",
    "    ('forest', 'graveyard'): 1.00, ('food', 'rooster'): 1.09, ('cemetery', 'woodland'): 1.18,\n",
    "    ('shore', 'voyage'): 1.22, ('bird', 'woodland'): 1.24, ('coast', 'hill'): 1.26,\n",
    "    ('furnace', 'implement'): 1.37, ('crane', 'rooster'): 1.41, ('hill', 'woodland'): 1.48,\n",
    "    ('car', 'journey'): 1.55, ('cemetery', 'mound'): 1.69, ('glass', 'jewel'): 1.78,\n",
    "    ('magician', 'oracle'): 1.82, ('crane', 'implement'): 2.37, ('brother', 'lad'): 2.41,\n",
    "    ('sage', 'wizard'): 2.46, ('oracle', 'sage'): 2.61, ('bird', 'crane'): 2.63,\n",
    "    ('bird', 'rock'): 2.63, ('food', 'fruit'): 2.69, ('brother', 'monk'): 2.74,\n",
    "    ('asylum', 'madhouse'): 3.04, ('furnace', 'stove'): 3.11, ('magician', 'wizard'): 3.21,\n",
    "    ('hill', 'mound'): 3.29, ('cord', 'string'): 3.41, ('glass', 'tumbler'): 3.45,\n",
    "    ('grin', 'smile'): 3.46, ('serf', 'slave'): 3.46, ('journey', 'voyage'): 3.58,\n",
    "    ('autograph', 'signature'): 3.59, ('coast', 'shore'): 3.60, ('forest', 'woodland'): 3.65,\n",
    "    ('implement', 'tool'): 3.66, ('cock', 'rooster'): 3.68, ('boy', 'lad'): 3.82,\n",
    "    ('cushion', 'pillow'): 3.84, ('cemetery', 'graveyard'): 3.88, ('automobile', 'car'): 3.92,\n",
    "    ('midday', 'noon'): 3.94, ('gem', 'jewel'): 3.94}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_list = []\n",
    "\n",
    "for key in P_dict:\n",
    "    if (key[0] not in vocab) or (key[1] not in vocab):\n",
    "        del_list.append(key)\n",
    "        \n",
    "for del_pair in del_list:\n",
    "    del P_dict[del_pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(len(P_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_list = list(P_dict.keys())\n",
    "P = set(P_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = []\n",
    "for key in P_dict:\n",
    "    S.append(P_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(len(S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 7: Calculate Cosine Similarities: </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_cos_sim(word_dict, info_matrix, vocab_list):\n",
    "    cos_sim_dict = {}\n",
    "    for key in word_dict:\n",
    "        i_w1 = vocab_list.index(key[0])\n",
    "        i_w2 = vocab_list.index(key[1])\n",
    "        cos_sim_dict[key] = cosine_similarity(sparse.csr_matrix(info_matrix[i_w1]), sparse.csr_matrix(info_matrix[i_w2]))\n",
    "        \n",
    "    return cos_sim_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Cosine Similarities for M1: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_M1 = cal_cos_sim(P_dict, M1, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Cosine Similarities for M1_plus: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_M1_plus = cal_cos_sim(P_dict, M1_plus, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Cosine Similarities for M2_10: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_M2_10 = cal_cos_sim(P_dict, M2_10, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Cosine Similarities for M2_100: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_M2_100 = cal_cos_sim(P_dict, M2_100, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Cosine Similarities for M2_300: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_M2_300 = cal_cos_sim(P_dict, M2_300, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Step 8: Calculate Pearson Correlation: </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Pearson Correlation for M1: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_list_M1 = []\n",
    "for key in S_M1:\n",
    "    S_list_M1.append(S_M1[key][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_M1, p_value_M1 = pearsonr(S_list_M1, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11843435038906105\n"
     ]
    }
   ],
   "source": [
    "print(corr_M1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Pearson Correlation for M1_plus: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_list_M1_plus = []\n",
    "for key in S_M1_plus:\n",
    "    S_list_M1_plus.append(S_M1_plus[key][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_M1_plus, p_value_M1_plus = pearsonr(S_list_M1_plus, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11993626872007863\n"
     ]
    }
   ],
   "source": [
    "print(corr_M1_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Pearson Correlation for M2_10: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_list_M2_10 = []\n",
    "for key in S_M2_10:\n",
    "    S_list_M2_10.append(S_M2_10[key][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_M2_10, p_value_M2_10 = pearsonr(S_list_M2_10, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07028427881086727\n"
     ]
    }
   ],
   "source": [
    "print(corr_M2_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Pearson Correlation for M2_100: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_list_M2_100 = []\n",
    "for key in S_M2_100:\n",
    "    S_list_M2_100.append(S_M2_100[key][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_M2_100, p_value_M2_100 = pearsonr(S_list_M2_100, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1143811990634159\n"
     ]
    }
   ],
   "source": [
    "print(corr_M2_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Pearson Correlation for M2_300: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_list_M2_300 = []\n",
    "for key in S_M2_300:\n",
    "    S_list_M2_300.append(S_M2_300[key][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_M2_300, p_value_M2_300 = pearsonr(S_list_M2_300, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14137103020616631\n"
     ]
    }
   ],
   "source": [
    "print(corr_M2_300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Word2Vec part: </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim import utils, matutils\n",
    "from numpy import dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_matrix = []\n",
    "\n",
    "for item in new_words:\n",
    "    w2v_matrix.append(model[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Cosine Similarities for w2v model: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_w2v = cal_cos_sim(P_dict, w2v_matrix, new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Pearson Correlation for w2v model: </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_list_w2v = []\n",
    "for key in S_w2v:\n",
    "    S_list_w2v.append(S_w2v[key][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_w2v, p_value_w2v = pearsonr(S_list_w2v, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7521151672039903\n"
     ]
    }
   ],
   "source": [
    "print(corr_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.849322751325408e-13\n"
     ]
    }
   ],
   "source": [
    "print(p_value_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19559\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [line.rstrip('\\n') for line in open('./word_test.v1.txt')]\n",
    "print(len(word_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8869\n",
      "10675\n"
     ]
    }
   ],
   "source": [
    "semantic_pairs = []\n",
    "syntactic_pairs = []\n",
    "split_line = re.compile(': gram1-adjective-to-adverb')\n",
    "wanted_line = re.compile('\\W.+')\n",
    "semantic = True\n",
    "\n",
    "for line in word_pairs:\n",
    "    if split_line.match(line):\n",
    "        semantic = False\n",
    "    if semantic and not wanted_line.match(line):\n",
    "        semantic_pairs.append(line)\n",
    "    elif semantic == False and not wanted_line.match(line):\n",
    "        syntactic_pairs.append(line)\n",
    "\n",
    "print(len(semantic_pairs))\n",
    "print(len(syntactic_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "2080\n"
     ]
    }
   ],
   "source": [
    "def filter_word_pairs(list_of_lines):\n",
    "    new_list = []\n",
    "    \n",
    "    for line in list_of_lines:\n",
    "        words = line.split()\n",
    "        if set(words).issubset(vocab):\n",
    "            new_list.append(line)\n",
    "    \n",
    "    return new_list\n",
    "            \n",
    "semantic_pairs = filter_word_pairs(semantic_pairs)\n",
    "print(len(semantic_pairs))\n",
    "syntactic_pairs = filter_word_pairs(syntactic_pairs)\n",
    "print(len(syntactic_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "1441\n"
     ]
    }
   ],
   "source": [
    "def analogy_test_w2v(list_of_lines, w2v_model):\n",
    "    successful = 0\n",
    "    \n",
    "    for line in list_of_lines:\n",
    "        words = line.split()\n",
    "        prediction = w2v_model.most_similar(positive=[words[3], words[0]], negative=[words[2]])[0]\n",
    "        \n",
    "        if prediction[0] == words[1]:\n",
    "            successful += 1\n",
    "                \n",
    "    return successful\n",
    "                \n",
    "print(analogy_test_w2v(semantic_pairs, model))\n",
    "print(analogy_test_w2v(syntactic_pairs, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_test_lsa(list_of_lines, M_lsa, vocab_list):\n",
    "    successful = 0\n",
    "    \n",
    "    for line in list_of_lines:\n",
    "        words = line.split()\n",
    "        prediction_vec = M_lsa[vocab_list.index(words[3])] - M_lsa[vocab_list.index(words[2])] + M_lsa[vocab_list.index(words[0])]\n",
    "        cos_sim_list = []\n",
    "        for word in vocab_list:\n",
    "            i_w = vocab_list.index(word)\n",
    "            cos_sim_list.append(cosine_similarity(sparse.csr_matrix(prediction_vec), sparse.csr_matrix(M_lsa[i_w])))\n",
    "        \n",
    "        if vocab_list[vocab_list.index(words[1])] == vocab_list[cos_sim_list.index(max(cos_sim_list))]:\n",
    "            successful += 1\n",
    "                \n",
    "    return successful\n",
    "\n",
    "print(analogy_test_lsa(semantic_pairs, M2_300, vocab))\n",
    "print(analogy_test_lsa(syntactic_pairs, M2_300, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0<br>\n",
    "0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
